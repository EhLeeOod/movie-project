{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4041a3f1-cfa8-4aee-a753-befd3d9467cc",
   "metadata": {},
   "source": [
    "# Project 2 - Part 7 (Core)\n",
    "\n",
    "For this project you will create a streamlit app to get predictions from your best model.\n",
    "\n",
    "### Deliverables:\r\n",
    "- \r\n",
    "New notebook file for Preparing for streamli- t\r\n",
    "Models and data sav- ed\r\n",
    "app.py in main folder of repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33e801-d3d5-4e2a-b5a6-7b6fd5e72251",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e34c997c-25d0-43dd-8dd8-2913363d3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "280a33e9-21c9-47a8-8a5b-cae0f18ac6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 16:21:01.105 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2024-01-26 16:21:01.109 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2024-01-26 16:21:01.112 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2024-01-26 16:21:01.200 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\eliud\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-01-26 16:21:01.202 No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "# functions\n",
    "\n",
    "# Define the load train or test data function with caching\n",
    "@st.cache_data\n",
    "def load_Xy_data(fpath):\n",
    "    return joblib.load(fpath)\n",
    "    \n",
    "@st.cache_resource\n",
    "def load_model_ml(fpath):\n",
    "    return joblib.load(fpath)\n",
    "\n",
    "@st.cache_data\n",
    "def load_network(fpath):\n",
    "    model = tf.keras.models.load_model(fpath)\n",
    "    return model\n",
    "\n",
    "@st.cache_data\n",
    "def load_lookup(fpath=FPATHS['Data']['ml']['target_lookup']):\n",
    "    return joblib.load(fpath)\n",
    "\n",
    "def predict_decode_deep(X_to_pred, network,lookup_dict,\n",
    "                       return_index=True):\n",
    "    \n",
    "    if isinstance(X_to_pred, str):\n",
    "        \n",
    "        X = [X_to_pred]\n",
    "    else:\n",
    "        X = X_to_pred\n",
    "    \n",
    "    pred_probs = network.predict(X)\n",
    "\n",
    "    pred_class = fn.convert_y_to_sklearn_classes(pred_probs)\n",
    "    \n",
    "    # Decode label\n",
    "    class_name = lookup_dict[pred_class[0]]\n",
    "\n",
    "    return class_name\n",
    "\n",
    "\n",
    "def classification_metrics_streamlit(y_true, y_pred, label='',\n",
    "                           figsize=(8,4),\n",
    "                           normalize='true', cmap='Blues',\n",
    "                           colorbar=False,values_format=\".2f\",\n",
    "                                    class_names=None):\n",
    "    \"\"\"Modified version of classification metrics function from Intro to Machine Learning.\n",
    "    Updates:\n",
    "    - Reversed raw counts confusion matrix cmap  (so darker==more).\n",
    "    - Added arg for normalized confusion matrix values_format\n",
    "    \"\"\"\n",
    "    # Get the classification report\n",
    "    report = classification_report(y_true, y_pred,target_names=class_names)\n",
    "    \n",
    "    ## Save header and report\n",
    "    header = \"-\"*70\n",
    "    final_report = \"\\n\".join([header,f\" Classification Metrics: {label}\", header,report,\"\\n\"])\n",
    "        \n",
    "    ## CONFUSION MATRICES SUBPLOTS\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
    "    \n",
    "    # Create a confusion matrix  of raw counts (left subplot)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=None, \n",
    "                                            cmap='gist_gray_r',# Updated cmap\n",
    "                                            display_labels = class_names, # Added display labels\n",
    "                                            values_format=\"d\", \n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[0]);\n",
    "    axes[0].set_title(\"Raw Counts\")\n",
    "    \n",
    "    # Create a confusion matrix with the data with normalize argument \n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=normalize,\n",
    "                                            cmap=cmap, \n",
    "                                            values_format=values_format, #New arg\n",
    "                                            display_labels = class_names, # Added display labels\n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[1]);\n",
    "    axes[1].set_title(\"Normalized Confusion Matrix\")\n",
    "    \n",
    "    # Adjust layout and show figure\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return final_report, fig\n",
    "\n",
    "def classification_metrics_streamlit_tensorflow(model,X_train=None, y_train=None, \n",
    "                                                label='Training Data',\n",
    "                                    figsize=(6,4), normalize='true',\n",
    "                                    output_dict = False,\n",
    "                                    cmap_train='Blues',\n",
    "                                    cmap_test=\"Reds\",\n",
    "                                    values_format=\".2f\", \n",
    "                                                class_names = None,\n",
    "                                    colorbar=False):\n",
    "    \n",
    "    ## Check if X_train is a dataset\n",
    "    if hasattr(X_train,'map'):\n",
    "        # If it IS a Datset:\n",
    "        # extract y_train and y_train_pred with helper function\n",
    "        y_train, y_train_pred = fn.get_true_pred_labels(model, X_train)\n",
    "    else:\n",
    "        # Get predictions for training data\n",
    "        y_train_pred = model.predict(X_train)\n",
    "\n",
    "\n",
    "     ## Pass both y-vars through helper compatibility function\n",
    "    y_train = fn.convert_y_to_sklearn_classes(y_train)\n",
    "    y_train_pred = fn.convert_y_to_sklearn_classes(y_train_pred)\n",
    "    \n",
    "    # Call the helper function to obtain regression metrics for training data\n",
    "    report, conf_mat = classification_metrics_streamlit(y_train, y_train_pred, \n",
    "                                                        figsize=figsize,\n",
    "                                         colorbar=colorbar, cmap=cmap_train, \n",
    "                                                        values_format=values_format,label=label,\n",
    "                                                       class_names=class_names)\n",
    "    return report, conf_mat\n",
    "\n",
    "X_train, y_train = load_Xy_data(fpath=FPATHS['Data']['ml']['train'])\n",
    "\n",
    "X_test, y_test = load_Xy_data(fpath=FPATHS['Data']['ml']['test'])\n",
    "\n",
    "def get_X_to_predict():\n",
    "    X_to_predict = pd.DataFrame({'bedrooms': selected_beds,\n",
    "                             'bathrooms': selected_baths, \n",
    "                             'sqft_lot': selected_lot},\n",
    "                               index=['House'])\n",
    "    return X_to_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b51301c-112c-429d-859f-6d9d936108dd",
   "metadata": {},
   "source": [
    "# Part 1: Preparing Best Models for Streamlit\n",
    "\n",
    "- Define a filepaths dictionary and save it to config/filepaths.json  to include file paths for each component you will save (review below).\n",
    "\n",
    "- Copy your best models from part 6 into the new notebook. \n",
    "\n",
    "- Update your code to define the final public-facing class labels. \n",
    "\n",
    "## Saving Your Models\n",
    "\n",
    "### For your Machine Learning model:\n",
    "\n",
    "- Save your training data  ([X_train, y_train]​)\n",
    "\n",
    "- Save your test data ([X_test, y_test]​)​\n",
    "\n",
    "- Save your target_lookup dictionary and/or your label encoder\n",
    "\n",
    "- Save your best model\n",
    "\n",
    "### For your Deep NLP model:\n",
    "\n",
    "- Save your training data  (train_ds​)\n",
    "\n",
    "- Save your test data (test_ds​)​\n",
    "\n",
    "- Save your best neural network.\n",
    "\n",
    "Reminder: use safe_format='tf' to save the model in a folder of repo-friendly files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d84a8-9416-4483-a178-da2e67d0fbda",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d67407-25d8-4e27-9e1b-1b14ddda7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9db25724-2e33-461d-b31e-3f4ad7086c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "import tensorflow as tf\n",
    "# Increase column width\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "import tensorflow as tf\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "368097de-28ac-4a76-9c50-c10ee7cdeb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "# ChatGPT function from LP\n",
    "import os\n",
    "def create_directories_from_paths(nested_dict):\n",
    "    \"\"\"OpenAI. (2023). ChatGPT [Large language model]. https://chat.openai.com \n",
    "    Recursively create directories for file paths in a nested dictionary.\n",
    "    Parameters:\n",
    "    nested_dict (dict): The nested dictionary containing file paths.\n",
    "    \"\"\"\n",
    "    for key, value in nested_dict.items():\n",
    "        if isinstance(value, dict):\n",
    "            # If the value is a dictionary, recurse into it\n",
    "            create_directories_from_paths(value)\n",
    "        elif isinstance(value, str):\n",
    "            # If the value is a string, treat it as a file path and get the directory path\n",
    "            directory_path = os.path.dirname(value)\n",
    "            # If the directory path is not empty and the directory does not exist, create it\n",
    "            if directory_path and not os.path.exists(directory_path):\n",
    "                os.makedirs(directory_path)\n",
    "                print(f\"Directory created: {directory_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cffb377-a6b8-47b1-b734-87ece71e26b5",
   "metadata": {},
   "source": [
    "#### Define a filepaths dictionary and save it to config/filepaths.json  to include file paths for each component you will save (review below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e92ed297-98fc-4447-8c94-51b29edece2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Data': {'ml': {'label_encoder': 'Data/label_encoder.joblib',\n",
      "                 'target_lookup': 'Data/target_lookup.joblib',\n",
      "                 'test': 'Data/test_data_ml.joblib',\n",
      "                 'train': 'Data/training_data_ml.joblib'},\n",
      "          'tf': {'test_tf': 'Data/test_ds.joblib',\n",
      "                 'train_tf': 'Data/train_ds.joblib'}},\n",
      " 'Models': {'clf': 'Models/clf.joblib', 'gru': 'Models/gru.joblib'}}\n"
     ]
    }
   ],
   "source": [
    "FPATHS = dict(\n",
    "\n",
    "    Data={\n",
    "        \"ml\":{\n",
    "            \"train\":\"Data/training_data_ml.joblib\",\n",
    "            \"test\":\"Data/test_data_ml.joblib\",\n",
    "            \"target_lookup\":\"Data/target_lookup.joblib\",\n",
    "            \"label_encoder\":\"Data/label_encoder.joblib\",\n",
    "         },\n",
    "        \"tf\":{\n",
    "            \"train_tf\":\"Data/train_ds.joblib\",\n",
    "            \"test_tf\":\"Data/test_ds.joblib\",\n",
    "        },\n",
    "    },\n",
    "\n",
    "    Models={\n",
    "        \"clf\":\"Models/clf.joblib\",\n",
    "        \"gru\":\"Models/gru.joblib\"}\n",
    ")\n",
    "pprint(FPATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ed4377c-de1a-46c9-aa43-4b2c516657c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function on your FPATHS dictionary\n",
    "create_directories_from_paths(FPATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "393fadcf-752c-4b46-8afb-d7c6ec45621c",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Save the filepaths \n",
    "import os, json\n",
    "os.makedirs('config/', exist_ok=True)\n",
    "FPATHS_FILE = 'config/filepaths.json'\n",
    "with open(FPATHS_FILE, 'w') as f:\n",
    "    json.dump(FPATHS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ee032-911f-4fcf-afa2-37a33a52bb51",
   "metadata": {},
   "source": [
    "#### Copy your best models from part 6 into the new notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c3bfb-ba40-4755-b0af-b435b29c90de",
   "metadata": {},
   "source": [
    "#### Update your code to define the final public-facing class labels. (Updated in Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99601ffd-9c67-4218-9376-fa002da7dd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Data': {'ml': {'train': 'Data/training_data_ml.joblib',\n",
       "   'test': 'Data/test_data_ml.joblib',\n",
       "   'target_lookup': 'Data/target_lookup.joblib',\n",
       "   'label_encoder': 'Data/label_encoder.joblib'},\n",
       "  'tf': {'train_tf': 'Data/train_ds.joblib',\n",
       "   'test_tf': 'Data/test_ds.joblib'}},\n",
       " 'Models': {'clf': 'Models/clf.joblib', 'gru': 'Models/gru.joblib'}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FPATHS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a4fbb-606f-4e16-9ef2-6878c879abc9",
   "metadata": {},
   "source": [
    "# Part 2: Streamlit App\n",
    "\n",
    "- Select either your best machine learning model or deep nlp model.\n",
    "\n",
    "- Create a streamlit app for getting predictions for a user-entered text from your loaded model\n",
    "\n",
    "- Include the ability to load the training and test data to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c1505-f212-4bee-9747-147f927b8859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
